([xem thêm tại đây](obsidian://open?vault=deep%20learning&file=2.%20Mathematical%20building%20block%20of%20neural%20network%2FCh%C3%BA%20th%C3%ADch%2FThao%20t%C3%A1c%20c%E1%BB%A7a%20tensor))
Các phép biến đổi mà mạng nơ-ron sâu học được có thể được giảm xuống thành một số ít các phép toán tensor áp dụng cho các tensor của dữ liệu số, tương tự như cách các chương trình máy tính có thể được giảm xuống thành các phép toán nhị phân.
### <span style="color:rgb(0, 176, 240)">1. Element-wise operations</span>
Phép toán `relu` và phép cộng là các phép toán theo từng phần tử: các phép toán được áp dụng độc lập cho từng mục trong các tensor đang xét => các phép toán này phù hợp với các triển khai song song quy mô lớn.

Dưới đây là một ví dụ về relu đơn giản:
```python
def naive_relu(x):
	assert len(x.shape) == 2 # x là mảng Numpy 2D
	x = x.copy() # copy để tránh viết đè lên tensor đầu vào
	for i in range(x.shape[0]):
		for j in range(x.shape[1]):
			x[i,j] = max(x[i,j], 0)
	return x
```
 và với phép cộng:
```python
def naive_add(x,y):
	assert len(x.shape) == 2
	assert len(x.shape) == y.shape
	x = x.copy()
	for i in range(x.shape[0]):
		for j in range(x.shape[1]):
			x[i,j] += y[i,j]
	return x
```

Ta có thể làm tương tự với các phép nhân, trừ, ... element-wise 

Trên thực tế thì các thao tác tensor trên đều được tích hợp và tối ưu sẵn trong thư viện NumPy arrays, các hàm này dựa theo quy trình BLAS - Basic Linear Algebra Subprograms. BLAS là các quy trình tensor operations ở mức thấp nhưng được triển khai bằng Fortran hoặc C để có hiệu suất cao và khả năng xử lý song song.
Ví dụ,  một phép toán element-wise có thể xử lý nhanh bằng:
```python
import numpy as np
z = x + y
z = np.maximum(z,0)
# tốn 0.00002 s
```
thay vì:
```python
z = naive_add(x,y)
z = naive_relu(z)
# tốn 0.00245 s
```

Khi sử dụng TensorFlow trên GPU, các thao tác theo từng phần tử được tăng tốc nhờ vào các triển khai CUDA được tối ưu hóa, giúp tận dụng kiến trúc song song của GPU.

### <span style="color:rgb(0, 176, 240)">2. Broadcasting</span> 
Ở trên ta có hàm naive_add() với 2 tensor bậc 2 cùng dạng, vậy nếu cộng 2 tensor khác dạng thì sẽ như thế nào?
Ví dụ:
Input:
```python
import tensorflow as tf
T1 = tf.constant([1,2,8], # shape: (2,3)
				[4,7,6])
T2 = tf.constant([7,8,10]) # shape: (1,3)
T3 = T1 + T2
print(T3)
```
Output:
```python
tf.Tensor(
	[[8, 10, 18]
	 [11, 15, 16]], shape=(2,3), dtype=int32)
```
Ở ví dụ này, tensor T2 được broadcasting để phù hợp với T1.
Broadcasting gồm 2 bước:
- B1: Trục được thêm vào tensor nhỏ để đồng bộ `ndim` với tensor lớn hơn
- B2: Tensor nhỏ được lặp lại theo chiều của trục để phù hợp với tensor lớn

### <span style="color:rgb(0, 176, 240)">3. Nhân tensor </span> 
Phép nhân tensor khác với phép nhân element-wise( * ) còn gọi là _dot product_.
Trong thư viện Numpy, phép nhân tensor sử dụng hàm `np.dot`, ví dụ:
```python
x = np.random.radom((32,))
y = np.random.random((32,))
z = np.dot(x,y)
```

Trong toán học thì sẽ được biểu diễn như sau: z = x • y 
Ví dụ về dot: 
```python
import numpy as geek

product = geek.dot(5, 4)

vector_a = 2 + 3j
vector_b = 4 + 5j

product = geek.dot(vector_a, vector_b)
print("Dot Product : ", product)
```
Output:
```lua
(-7+22j)
```

Ví dụ 2:
```python
import numpy as geek

vector_a = geek.array([[1, 4], [5, 6]])
vector_b = geek.array([[2, 4], [5, 2]])

product = geek.dot(vector_a, vector_b)
print("Dot Product : \n", product)

product = geek.dot(vector_b, vector_a)
print("\nDot Product : \n", product)
```
Output:
```lua
Dot Product  : 
 [[22 12]
 [40 32]]

Dot Product  : 
 [[22 32]
 [15 32]]
```

### <span style="color:rgb(0, 176, 240)">4. Tensor reshaping</span>

Phép biến đổi reshape trong TensorFlow cho phép bạn thay đổi hình dạng của một tensor mà không thay đổi dữ liệu của nó. Điều này hữu ích khi bạn cần thay đổi cấu trúc của tensor để phù hợp với đầu vào của các lớp mạng nơ-ron khác nhau hoặc các phép toán khác.

Ví dụ, giả sử tensor 1D với 12 phần tử, chuyển nó thành một tensor 2D có kích thước (3, 4):

```python
import tensorflow as tf

# Tạo một tensor 1D với 12 phần tử
x = tf.constant([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])

# Reshape thành tensor 2D có kích thước (3, 4)
x_reshaped = tf.reshape(x, (3, 4))

print(x_reshaped.numpy())
```

Kết quả sẽ là:

```lua
[[ 1  2  3  4]
 [ 5  6  7  8]
 [ 9 10 11 12]]
```

Đây là một ví dụ cơ bản về cách sử dụng `tf.reshape` để thay đổi hình dạng của tensor từ (12,) thành (3, 4). Phép biến đổi reshape là một trong những công cụ quan trọng trong TensorFlow để làm việc với các dữ liệu có cấu trúc khác nhau.

### <span style="color:rgb(0, 176, 240)">5. Giải thích cách hoạt động của các tensor operation bằng hình học</span> 

### <span style="color:rgb(0, 176, 240)">6. Giải thích học sâu bằng hình học</span> 
Trong deep learning, <span style="color:rgb(66, 255, 242)">mạng neural được hiểu như là một chuỗi các phép biến đổi tensor đơn giản của dữ liệu đầu vào</span>. Mỗi phép biến đổi này là một <span style="color:rgb(66, 255, 242)">phép biến đổi hình học trong không gian</span> có số chiều cao, và tổng hợp của chúng tạo thành một biến đổi hình học phức tạp.

Hình dung trong không gian ba chiều có thể giúp bạn hiểu hơn: hãy tưởng tượng hai tờ giấy màu, một màu đỏ và một màu xanh, đặt chúng lên nhau và nhào nặn chúng lại thành một quả bóng giấy nhỏ. Quả bóng giấy này tượng trưng cho dữ liệu đầu vào của bạn, với mỗi tờ giấy biểu thị một lớp dữ liệu trong bài toán phân loại. <span style="color:rgb(66, 255, 242)">Mục đích của mạng neural là tìm ra một phép biến đổi của quả bóng giấy để giải nén nó, làm cho hai lớp dữ liệu trở nên dễ phân biệt hơn</span>. Trên thực tế, deep learning thực hiện phép biến đổi này bằng cách thực hiện một chuỗi các phép toán đơn giản trong không gian ba chiều, tương tự như cách bạn có thể sử dụng ngón tay của mình để giải nén quả bóng giấy một lần di chuyển một lần.

Mục đích của học máy là giải nén các cấu trúc dữ liệu phức tạp, như các "bề mặt" liên tục như tờ giấy bị nhào nặn này, trong không gian có số chiều cao. Deep learning xuất sắc trong nhiệm vụ này bởi vì nó phân rã một phép biến đổi hình học phức tạp thành một chuỗi các bước đơn giản—một chiến lược tương tự như cách con người giải nén một quả bóng giấy. Mỗi lớp trong mạng neural sử dụng một phép biến đổi để từ từ làm rõ ràng dữ liệu, và hiệu ứng tổng hợp của nhiều lớp cho phép xử lý các quá trình phân giải mạch lạc vô cùng phức tạp.